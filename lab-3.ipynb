{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex\n",
    "import re\n",
    "\n",
    "# pandas + numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# setting pandas options\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n",
    "# storing and loading models\n",
    "import pickle\n",
    "\n",
    "# to set types for functions\n",
    "from typing import Tuple\n",
    "\n",
    "# Plotting\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "# gpu debug\n",
    "import torch\n",
    "\n",
    "# setting device to use GPU for NLP backend if you have GPU available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# SBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# UMAP\n",
    "from umap import UMAP\n",
    "\n",
    "#HDBSCAN\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# topic finding\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Loading model from pickle if possible, to avoid downloading it again\n",
    "try:\n",
    "    model = pickle.load(open(f'model-{device}.pkl', 'rb'))\n",
    "\n",
    "    model_load = True\n",
    "\n",
    "except:\n",
    "    model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
    "    pickle.dump(model, open(f'model-{device}.pkl', 'wb'))\n",
    "\n",
    "    model_load = False\n",
    "\n",
    "print(f\"\"\"\n",
    "GPUs detected:          {torch.cuda.device_count()}\n",
    "Using GPU:              {torch.cuda.is_available()}\n",
    "Device:                 {device}\n",
    "Got model from pickle:  {model_load}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_most_relevant_word(input: list, num_words=5) -> list:\n",
    "  \"\"\"\n",
    "  Function that finds the most relevant words per cluster id.\n",
    "\n",
    "  Args:\n",
    "      input (list): A list of title strings aggregated by cluster id.\n",
    "      num_words (int, optional): How many words you want. Defaults to 5.\n",
    "\n",
    "  Returns:\n",
    "      list: Returns a list of most relevant words, with lenght of unique cluster Ids\n",
    "  \"\"\"\n",
    "\n",
    "  most_relevant_words = []\n",
    "  \n",
    "  for corpus in input:\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    importance = np.argsort(np.asarray(X.sum(axis=0)).ravel())[::-1]\n",
    "    tfidf_feature_names = np.array(vectorizer.get_feature_names_out()) # get_feature_names\n",
    "    most_relevant_words.append(tfidf_feature_names[importance[:num_words]])\n",
    "\n",
    "  return most_relevant_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_cleaner(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Function to clean up strings.\n",
    "\n",
    "    Args:\n",
    "        input (str): String to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # turning lowercase\n",
    "    input = input.lower()\n",
    "\n",
    "    # removing punctuation and other non-alphanumeric characters\n",
    "    input = re.sub(r'[^\\w\\s]', '', input)\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_most_relevant_word(input: list, num_words=5) -> list:\n",
    "  \"\"\"\n",
    "  Function that finds the most relevant words per cluster id.\n",
    "\n",
    "  Args:\n",
    "      input (list): A list of title strings aggregated by cluster id.\n",
    "      num_words (int, optional): How many words you want. Defaults to 5.\n",
    "\n",
    "  Returns:\n",
    "      list: Returns a list of most relevant words, with lenght of unique cluster Ids\n",
    "  \"\"\"\n",
    "\n",
    "  most_relevant_words = []\n",
    "  \n",
    "  for corpus in input:\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    importance = np.argsort(np.asarray(X.sum(axis=0)).ravel())[::-1]\n",
    "    tfidf_feature_names = np.array(vectorizer.get_feature_names_out()) # get_feature_names\n",
    "    most_relevant_words.append(tfidf_feature_names[importance[:num_words]])\n",
    "\n",
    "  return most_relevant_words\n",
    "\n",
    "\n",
    "\n",
    "def topic_by_clusterId(result: pd.DataFrame) -> dict:\n",
    "  \"\"\"\n",
    "  Function that maps topics to cluster ids.\n",
    "\n",
    "  Args:\n",
    "      result (pd.DataFrame): Dataframe with cluster ids and topics.\n",
    "\n",
    "  Returns:\n",
    "      dict: Dictionary with cluster ids as keys and topics as values.\n",
    "  \"\"\"\n",
    "\n",
    "  #print(result.isna().sum())\n",
    "\n",
    "  df_group = result[[\"titles\", \"cluster_label\"]].groupby(\"cluster_label\").agg(list).reset_index()\n",
    "\n",
    "  df_group[\"topics\"] = tfidf_most_relevant_word(df_group[\"titles\"])\n",
    "\n",
    "  return dict(zip(df_group.cluster_label, df_group.topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you actually cast the type here, then it works with how pandas casts types and you don't have to worry about copying seriers\n",
    "def result_df_maker(embeddings: np.ndarray, cluster_labels: np.ndarray, titles: np.ndarray) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Function to make a dataframe with the embeddings, cluster labels, topic per cluster label and titles.\n",
    "\n",
    "  Args:\n",
    "      embeddings (np.ndarray): 2D array of embeddings.\n",
    "      cluster_labels (np.ndarray): array of cluster labels.\n",
    "      titles (np.ndarray): array of titles.\n",
    "\n",
    "  Returns:\n",
    "      pd.DataFrame: Dataframe with embeddings, cluster labels, topics per cluster, and titles.\n",
    "  \"\"\"\n",
    "  result = pd.DataFrame(embeddings, columns=['x', 'y'])\n",
    "\n",
    "  result[\"titles\"] = titles\n",
    "\n",
    "  result[\"cluster_label\"] = cluster_labels\n",
    "\n",
    "  topic_dict = topic_by_clusterId(result)\n",
    "\n",
    "  result[\"topics\"] = result[\"cluster_label\"].apply(lambda x: topic_dict[x])\n",
    "\n",
    "  result[\"topics\"] = result[\"topics\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "  return result\n",
    "\n",
    "def result_splitter(result: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "  \"\"\"\n",
    "  Function to split the dataframe into two dataframes, one for clustered and one for outliers.\n",
    "\n",
    "  Args:\n",
    "      result (pd.DataFrame): Dataframe with embeddings, cluster labels, topics per cluster, and titles.\n",
    "\n",
    "  Returns:\n",
    "      Tuple[np.ndarray, np.ndarray]: Tuple of two dataframes, one for clustered and one for outliers.\n",
    "  \"\"\"\n",
    "\n",
    "  clustered = result.loc[result.cluster_label != -1, :]\n",
    "  outliers = result.loc[result.cluster_label == -1, :]\n",
    "  return clustered, outliers\n",
    "\n",
    "# the cavalry is not here, but it's fine! Why? I am here!\n",
    "def result_tracer(clustered: pd.DataFrame, outliers: pd.DataFrame) -> Tuple[go.Scattergl, go.Scattergl]:\n",
    "  \"\"\"\n",
    "  Function to make a scatter traces of the clustered and outliers.\n",
    "\n",
    "  Args:\n",
    "      clustered (pd.DataFrame): clustered dataframe to be colored by cluster and get hover data\n",
    "      outliers (pd.DataFrame): outlier data frame with grey color and no hover data\n",
    "\n",
    "  Returns:\n",
    "      Tuple[go.Scattergl, go.Scattergl]: Tuple of two scatter traces.\n",
    "  \"\"\"\n",
    "\n",
    "  trace_cluster = go.Scattergl(\n",
    "    x=clustered.x, \n",
    "    y=clustered.y, \n",
    "    mode=\"markers\", \n",
    "    name=\"Clustered\",\n",
    "\n",
    "    # styling markers\n",
    "    marker=dict(\n",
    "      size=2, \n",
    "      color=clustered.cluster_label,\n",
    "      colorscale=\"Rainbow\"\n",
    "    ), \n",
    "\n",
    "    # setting hover text to the titles of the videos\n",
    "    hovertemplate=\"<b>Topics:</b> %{customdata[0]} <br><b>Cluster Id:</b> %{customdata[1]}<extra></extra>\", \n",
    "    customdata=np.column_stack([clustered.topics, clustered.cluster_label]),\n",
    "  )\n",
    "\n",
    "  trace_outlier = go.Scattergl(\n",
    "    x=outliers.x,\n",
    "    y=outliers.y,\n",
    "    mode=\"markers\",\n",
    "    name=\"Outliers\",\n",
    "\n",
    "    marker=dict(\n",
    "      size=1,\n",
    "      color=\"grey\"\n",
    "    ),\n",
    "\n",
    "    hovertemplate=\"Outlier<extra></extra>\"\n",
    "  )\n",
    "\n",
    "  return trace_cluster, trace_outlier\n",
    "\n",
    "def result_tracer_wrapper(uembs: np.ndarray, cluster_labels: np.ndarray, titles: np.ndarray) -> Tuple[go.Scattergl, go.Scattergl]:\n",
    "  \"\"\"\n",
    "  Function to make a scatter traces of the clustered and outliers.\n",
    "\n",
    "  Args:\n",
    "      uembs (np.ndarray): 2D array of embeddings.\n",
    "      cluster_labels (np.ndarray): array of cluster labels.\n",
    "      titles (np.ndarray): array of titles.\n",
    "\n",
    "  Returns:\n",
    "      Tuple[go.Scattergl, go.Scattergl]: Tuple of two scatter traces.\n",
    "  \"\"\"\n",
    "\n",
    "  result = result_df_maker(uembs, cluster_labels, titles)\n",
    "  clustered, outliers = result_splitter(result)\n",
    "  trace_cluster, trace_outlier = result_tracer(clustered, outliers)\n",
    "  return trace_cluster, trace_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplotter(trace_nested_list: list, titles: list, base_size=1000) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Function to make a figure with subplots of the clustered and outliers.\n",
    "\n",
    "    Args:\n",
    "        trace_nested_list (list): list holding rows of columns, each column holding traces. \n",
    "        titles (list): Titles for the subplots\n",
    "        base_size (int, optional): Base size of the sub plots. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        go.Figure: Figure with subplots.\n",
    "    \"\"\"\n",
    "    \n",
    "    row_count = len(trace_nested_list)\n",
    "    col_count = len(trace_nested_list[0])\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=row_count, \n",
    "        cols=col_count,\n",
    "        subplot_titles=(titles),\n",
    "        vertical_spacing=0.02,\n",
    "        horizontal_spacing=0.02\n",
    "    )\n",
    "\n",
    "    for i, row in enumerate(trace_nested_list):\n",
    "        for j, col in enumerate(row):\n",
    "\n",
    "            # adding both outlieers and clustered\n",
    "            for trace in col:\n",
    "                fig.add_trace(trace, row=i+1, col=1)\n",
    "    \n",
    "    # figure settings\n",
    "    fig.update_xaxes(visible=False)\n",
    "    fig.update_yaxes(visible=False)\n",
    "    \n",
    "    fig.update_layout(width=base_size*col_count, height=base_size*row_count, plot_bgcolor='rgba(250,250,250,1)')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving / Showing Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_show_save(fig: go.Figure, filename: str, show=True):\n",
    "  \"\"\"\n",
    "  Function to show and save a figure.\n",
    "\n",
    "  Args:\n",
    "      fig (go.Figure): fig to be saved and shown\n",
    "      filename (str): filename to save the figure, without extension\n",
    "      show (bool, optional): Option to disable showing of figure (in case too big for notebook). Defaults to True.\n",
    "  \"\"\"\n",
    "  \n",
    "  # writing both interactible .html and static image .png\n",
    "  fig.write_html(f\"figures/{filename}.html\")\n",
    "  fig.write_image(f\"figures/{filename}.png\")\n",
    "\n",
    "  if show: \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got data from kaggle: https://www.kaggle.com/datasets/datasnaek/youtube-new?resource=download\n",
    "\n",
    "df_whole = pd.read_csv(\"data/USvideos.csv\")\n",
    "\n",
    "df = \n",
    "\n",
    "# if your computer does not have GPU support, you can use a sample of the dataset instead to make it run in a reasonable time\n",
    "# if you want to use the full dataset even wihtout GPU in case you have a very strong CPU, then you can just comment out the next line\n",
    "if device == \"cpu\": df = df.sample(frac=0.05)\n",
    "\n",
    "print(df_whole.shape)\n",
    "\n",
    "df_whole.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_clean\"] = \n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = \n",
    "\n",
    "print(f\"The shape of our embeddings: {embs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensinality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering 2D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing topic per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('intro-ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef24d37fd96e52a10a8ffec0b63953b9c36537411c36f04f2b5bba2ba7dc4cf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
